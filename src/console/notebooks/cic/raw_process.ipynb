{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "csv_fname = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_output_if_exists = True\n",
    "self_classify = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing with pre-determined classifications\n",
    "classifications = f\"\"\"\n",
    "| Classification | Description |\n",
    "|---|---|\n",
    "| Integration | Key issues include difficulting integrating capabilities of cloud services into solutions, ensuring seamless interoperability. |\n",
    "| Breadth | Key issues include navigating an overwhelming range of service options. |\n",
    "| Containers | Key issues include challenges in container orchestration, ensuring compatibility in containerized environments.\n",
    "| Compute | Key issues include optimizing compute resources (e.g., VMs, containers, serverless) for performance and efficient resource utilization. |\n",
    "| Ease | Key issues include steep learning curves and inconsistent user experiences with cloud services. |\n",
    "| Portal | Key issues include cumbersome navigation, non-intuitive interfaces, and a lack of intuitive design. |\n",
    "| High Cost / Pricing | Key issues include unexpected costs, complex pricing models, and difficulty in predicting costs. |\n",
    "| Skill | Key issues include gaps in technical expertise, steep training requirements, and the need for specialized skills. |\n",
    "| Customer Support | Key issues include delayed support response times, inconsistent service quality, and limited access to knowledgeable support resources.\n",
    "| Migration | Key issues include navigating the complexities of migrating workloads between cloud providers, from on-premises environments, or hybrid environments. |\n",
    "| Privacy | Key issues include data confidentiality concerns, compliance with data protection regulations, and ensuring data security. |\n",
    "| Database | Key issues include database performance bottlenecks, data scalability challenges, and managing complex database configurations. |\n",
    "| Documentation | Key issues include outdated or incomplete documentation, lack of examples, and difficulty in finding relevant information. |\n",
    "| Getting Started | Key issues include overwhelming setup procedures, unclear onboarding processes, and lack of guidance for new users. |\n",
    "| AI | Key issues include integration challenges with AI into existing solutions, ensuring AI models are accurate and efficient, and managing AI workloads. |\n",
    "| Missing Features | Key issues include the absence of critical features, limitations in functionality, and gaps in service offerings. |\n",
    "| Reliability | Key issues include service outages, inconsistent performance, and lack of redundancy in cloud services. |\n",
    "| Innovate | Key issues include slow adoption of cutting-edge technologies, limited support for emerging features, and difficulty integrating new technologies. |\n",
    "| Automation | Key issues include complexities in automating cloud services, ensuring automation scripts are reliable, and managing automated workflows. |\n",
    "| Scaling | Key issues include challenges in scaling resources to meet demand, optimizing resource allocation, and ensuring performance at scale. |\n",
    "| Lock-In | Key issues include dependency on proprietary cloud services, challenges in migrating away from cloud providers, and concerns about vendor lock-in. |\n",
    "| Select Service | Key issues include confusion from an abundance of similar services, difficulty in selecting the right service for specific use cases, and lack of clear differentiation between services. |\n",
    "| Networking | Key issues include complex network configurations, challenges in optimizing network performance, and managing network security. |\n",
    "| IAM | Key issues include managing intricate identity and access management policies, ensuring secure access controls, and preventing unauthorized access. |\n",
    "| Monitor | Key issues include setting up comprehensive observability, integrating effective logging and monitoring solutions, and ensuring timely alerts.\n",
    "| OS | Key issues include compatibility issues with operating systems, managing OS configurations, and ensuring OS security. |\n",
    "| Multi-Cloud | Key issues include interoperability challenges between cloud providers, managing multiple cloud environments, and ensuring consistent performance across clouds. |\n",
    "| None | No challenges. |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "wdir = os.path.abspath('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from dotenv import dotenv_values\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai.types.chat.chat_completion import ChatCompletionMessage\n",
    "from openai.types.chat.chat_completion_system_message_param import ChatCompletionSystemMessageParam\n",
    "from openai.types.chat.chat_completion_user_message_param import ChatCompletionUserMessageParam\n",
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = dotenv_values(f\"{wdir}/configuration/.env\")\n",
    "notebooks_path = os.path.join(wdir, \"src/console/notebooks/cic/\")\n",
    "\n",
    "csv_fpath = os.path.join(notebooks_path, \"input\", csv_fname)\n",
    "output_dir = os.path.join(notebooks_path, \"output\", os.path.splitext(os.path.basename(csv_fname))[0])\n",
    "classifications_output_dir = os.path.join(output_dir, \"self_classifications\" if self_classify else \"classifications\")\n",
    "\n",
    "credential = DefaultAzureCredential(\n",
    "    exclude_workload_identity_credential=True,\n",
    "    exclude_developer_cli_credential=True,\n",
    "    exclude_environment_credential=True,\n",
    "    exclude_managed_identity_credential=True,\n",
    "    exclude_powershell_credential=True,\n",
    "    exclude_shared_token_cache_credential=True,\n",
    "    exclude_interactive_browser_credential=True\n",
    ")\n",
    "\n",
    "openai_token_provider = get_bearer_token_provider(credential, 'https://cognitiveservices.azure.com/.default')\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=env_vars[\"AOAI_ENDPOINT\"],\n",
    "    azure_ad_token_provider=openai_token_provider,\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing Helpers\n",
    "class Stopwatch:\n",
    "    elapsed = 0\n",
    "    is_running = False\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.stop()\n",
    "\n",
    "    def reset(self):\n",
    "        self.elapsed = 0\n",
    "        self.is_running = False\n",
    "\n",
    "    def start(self):\n",
    "        if self.is_running:\n",
    "            return\n",
    "\n",
    "        self.is_running = True\n",
    "        self.start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        if not self.is_running:\n",
    "            return\n",
    "\n",
    "        self.is_running = False\n",
    "        self.elapsed = time.perf_counter() - self.start_time\n",
    "\n",
    "    def get_current_elapsed(self):\n",
    "        if not self.is_running:\n",
    "            return self.elapsed\n",
    "\n",
    "        self.elapsed = time.perf_counter() - self.start_time\n",
    "        return self.elapsed\n",
    "\n",
    "\n",
    "# Storage Helpers\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj, 'to_dict'):\n",
    "            return obj.to_dict()\n",
    "        if hasattr(obj, 'as_dict'):\n",
    "            return obj.as_dict()\n",
    "        if hasattr(obj, 'model_dump'):\n",
    "            return obj.model_dump()\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def create_directory(dir: str, clear_if_not_empty: bool = False) -> str:\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "    if clear_if_not_empty:\n",
    "        for file in os.listdir(dir):\n",
    "            file_path = os.path.join(dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "\n",
    "    return dir\n",
    "\n",
    "\n",
    "def create_json_file(fpath: str, data: any, indent: int = 4) -> None:\n",
    "    if not os.path.exists(os.path.dirname(fpath)):\n",
    "        create_directory(os.path.dirname(fpath))\n",
    "\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(data, f, indent=indent, cls=CustomEncoder)\n",
    "        \n",
    "\n",
    "# OpenAI Helpers\n",
    "def get_embedding(text: str):\n",
    "    embedding_response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=env_vars[\"EMBEDDING_DEPLOYMENTNAME\"]\n",
    "    )\n",
    "    return embedding_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_classify_prompt = \"\"\"You are a helpful AI assistant for determining a list of key observations from attributions and verbatims from customers of cloud providers (e.g., Azure, AWS, GCP).\n",
    "The key observations will be used to help program managers, product owners, and engineers understand the most common issues and opportunities for improvement in their products and services.\n",
    "\n",
    "## On your ability to determine key observations\n",
    "\n",
    "- Use the provided customer project data to determine the top key observations, regardless of your own knowledge or information.\n",
    "- Key observations should be specific and detailed to provide the most value.\n",
    "- Ensure the key observations are based on the evidence provided.\n",
    "- Each observation should be unique.\n",
    "- There is no limit to the number of key observations you can provide.\n",
    "\n",
    "## Key Observation Examples\n",
    "\n",
    "{{\n",
    "  \"short_name\": \"High Cost\",\n",
    "}}\n",
    "\n",
    "{{\n",
    "  \"short_name\": \"Containers\",\n",
    "}}\n",
    "\n",
    "{{\n",
    "  \"short_name\": \"AI\",\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_prompt = \"\"\"Using the provided classification list, classify the attributions and verbatims from customers of cloud providers (e.g., Azure, AWS, GCP) into one or more of the classifications based on the given facts and data.\n",
    "\n",
    "## On your ability to classify\n",
    "\n",
    "- Use the data provided to classify, regardless of your own knowledge or information.\n",
    "- You must only use the classification list provided.\n",
    "- Ensure the data classifications are based on facts, both quantitative and qualitative.\n",
    "- You must only classify data based on known facts. Do not assume or provide indication that a classification is associated if detail is not provided.\n",
    "- Only return relevant data classifications that you are highly confident of.\n",
    "- There is no limit to the number of data classifications you can provide.\n",
    "- If no classification is applicable, provide an empty list. Ignore classifications that are not applicable.\n",
    "\n",
    "## Classifications\n",
    "\n",
    "{classifications}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_prompt = \"\"\"Below I will present you with a user's request, and potential relevant context to help you solve it.\n",
    "\n",
    "Based on the user's request, use the context to answer the following survey to the best of your ability.\n",
    "\n",
    "Here is the user's request:\n",
    "\n",
    "```\n",
    "{task}\n",
    "```\n",
    "\n",
    "Here is the context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Here is the survey:\n",
    "\n",
    "1. List any specific facts or figures that are GIVEN based on the request. It is possible that there are none.\n",
    "2. List any facts that are recalled from memory, your knowledge, or well-reasoned assumptions, etc.\n",
    "\n",
    "When answering this survey, keep in mind that facts will typically be specific details.\n",
    "Provide as many facts as you can, even if they seem trivial or unimportant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = \"\"\"Context: {context}\n",
    "\n",
    "Facts: {facts}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation(BaseModel):\n",
    "    short_name: str = Field(description=\"Short name of the key observation/challenge.\")\n",
    "    key_insights: List[str] = Field(\n",
    "        description=\"Detailed list of the key insights that support the observation/challenge, including qualitative and quantitative data.\")\n",
    "    \n",
    "class Observations(BaseModel):\n",
    "    observations: List[Observation] = Field(description=\"List of classifications.\")\n",
    "    \n",
    "class RequestFactsModel(BaseModel):\n",
    "    given_facts: Optional[List[str]] = Field(\n",
    "        description=\"Any specific facts or figures that are GIVEN based on the request. It is possible that there are none.\")\n",
    "    recalled_facts: Optional[List[str]] = Field(\n",
    "        description=\"any facts that are recalled from memory, your knowledge, or well-reasoned assumptions, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "if reload_output_if_exists and os.path.exists(os.path.join(output_dir)):\n",
    "    for f in os.listdir(output_dir):\n",
    "        if f.endswith(\".json\"):\n",
    "            with open(os.path.join(output_dir, f), 'r') as file:\n",
    "                data.append(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data) == 0:    \n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(csv_fpath, chunksize=10)\n",
    "        \n",
    "        for i, df in enumerate(chunk_iterator):            \n",
    "            for j, row in df.iterrows():\n",
    "                attribution = [\n",
    "                    row[\"BrandAssigned\"],\n",
    "                    row[\"SAM11\"],\n",
    "                    row[\"TAXALN\"],\n",
    "                    \"Other\" if row[\"Q005\"].startswith(\"Other\") else row[\"Q005\"],\n",
    "                    row[\"Q005_996_TEXT\"] if row[\"Q005\"].startswith(\"Other\") else \"\",\n",
    "                    row[\"Q009_\"],\n",
    "                    row[\"Cloud_Usage\"],\n",
    "                    row[\"Q082\"],\n",
    "                    row[\"Q048b\"],\n",
    "                    \"\" if row[\"Q089a_2\"] == \"#NULL!\" else \"ISV\",\n",
    "                    \"\" if (row[\"Q102a\"] == \"No\" or row[\"Q102a\"] == \"#NULL!\") else \"Startup\"\n",
    "                ]\n",
    "                \n",
    "                dp = {\n",
    "                    \"response_id\": row[\"ResponseId\"],\n",
    "                    \"attribution\": \", \".join(attribution),\n",
    "                    \"verbatim\": str(row[\"Q024b\"]),\n",
    "                }\n",
    "                \n",
    "                dp[\"embedding\"] = get_embedding(dp[\"verbatim\"])\n",
    "                data.append(dp)\n",
    "                \n",
    "                create_json_file(os.path.join(output_dir, f\"{dp['response_id']}.json\"), dp)\n",
    "                \n",
    "            break; # Exiting based on the first chunk for testing purposes\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dp_observations = []\n",
    "processed_response_ids = []\n",
    "\n",
    "if reload_output_if_exists and os.path.exists(classifications_output_dir):\n",
    "    for f in os.listdir(classifications_output_dir):\n",
    "        if f.endswith(\".json\"):\n",
    "            with open(os.path.join(classifications_output_dir, f), \"r\") as file:\n",
    "                dp_observations = json.load(file)\n",
    "                all_dp_observations.extend(dp_observations)\n",
    "                processed_response_ids.append(f.split(\".\")[0])\n",
    "                \n",
    "filtered_data = [d for d in data if d[\"response_id\"] not in processed_response_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dp in filtered_data:\n",
    "    with Stopwatch() as sw:\n",
    "        prompt_tokens = 0\n",
    "        completion_tokens = 0    \n",
    "\n",
    "        dp_json = json.dumps({\n",
    "            \"attribution\": dp[\"attribution\"],\n",
    "            \"verbatim\": dp[\"verbatim\"]\n",
    "        }, cls=CustomEncoder)\n",
    "        \n",
    "        # 1 - Determine any facts from the data\n",
    "        context_content = context_prompt.format(context=dp_json, facts=\"To be determined\")\n",
    "        \n",
    "        planning_messages = [ChatCompletionUserMessageParam(role=\"user\", content=fact_prompt.format(task=self_classify_prompt, context=context_content))]\n",
    "        \n",
    "        fact_completion = openai_client.beta.chat.completions.parse(\n",
    "            model=env_vars[\"CHATCOMPLETION_DEPLOYMENTNAME\"],\n",
    "            messages=planning_messages,\n",
    "            response_format=RequestFactsModel,\n",
    "            temperature=0.3,\n",
    "            top_p=0.3\n",
    "        )\n",
    "        \n",
    "        facts = fact_completion.choices[0].message.parsed.model_dump()\n",
    "        prompt_tokens += fact_completion.usage.prompt_tokens\n",
    "        completion_tokens += fact_completion.usage.completion_tokens\n",
    "\n",
    "        context_content = context_prompt.format(context=dp_json, facts=facts)\n",
    "\n",
    "        if self_classify:\n",
    "            # 2 - Perform self-classification of the input data\n",
    "            execute_messages = [\n",
    "                ChatCompletionSystemMessageParam(role=\"system\", content=self_classify_prompt),\n",
    "                ChatCompletionUserMessageParam(role=\"user\", content=context_content)\n",
    "            ]\n",
    "\n",
    "            observation_completion = openai_client.beta.chat.completions.parse(\n",
    "                model=env_vars[\"CHATCOMPLETION_DEPLOYMENTNAME\"],\n",
    "                messages=execute_messages,\n",
    "                response_format=Observations,\n",
    "                temperature=0.3,\n",
    "                top_p=0.3\n",
    "            )\n",
    "        else:\n",
    "            # 2 - Classify the input data based on the provided classifications\n",
    "            execute_messages = [\n",
    "                ChatCompletionSystemMessageParam(role=\"system\", content=classify_prompt.format(classifications=classifications)),\n",
    "                ChatCompletionUserMessageParam(role=\"user\", content=context_content)\n",
    "            ]\n",
    "            \n",
    "            observation_completion = openai_client.beta.chat.completions.parse(\n",
    "                model=env_vars[\"CHATCOMPLETION_DEPLOYMENTNAME\"],\n",
    "                messages=execute_messages,\n",
    "                response_format=Observations,\n",
    "                temperature=0.3,\n",
    "                top_p=0.3\n",
    "            ) \n",
    "            \n",
    "        dp_observations = observation_completion.choices[0].message.parsed.model_dump()\n",
    "        prompt_tokens += observation_completion.usage.prompt_tokens\n",
    "        completion_tokens += observation_completion.usage.completion_tokens\n",
    "\n",
    "        # 3 - Get similarity score between the verbatim and the observations            \n",
    "        for observation in dp_observations[\"observations\"]:\n",
    "            observation[\"embedding\"] = get_embedding(observation[\"short_name\"])\n",
    "            similarity = cosine_similarity([dp[\"embedding\"]], [observation[\"embedding\"]])\n",
    "            observation[\"similarity\"] = round(similarity[0][0], 2)\n",
    "        \n",
    "        execution_time = sw.get_current_elapsed()\n",
    "        \n",
    "        dp_observations[\"execution_time\"] = execution_time\n",
    "        dp_observations[\"prompt_tokens\"] = prompt_tokens\n",
    "        dp_observations[\"completion_tokens\"] = completion_tokens\n",
    "        \n",
    "        all_dp_observations.append(dp_observations)\n",
    "        \n",
    "        create_json_file(os.path.join(classifications_output_dir, f\"{dp['response_id']}.json\"), dp_observations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
